{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification_15-Jun.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3382baf0eb914c7aba7a9ce8c2c78c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d0bd035a01eb4547bd84f7de6419fc76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d876bd69e91d4a1a8ca31ccdeb0889f6",
              "IPY_MODEL_ff1b8ddef059409cb67c7cb23859bfa0"
            ]
          }
        },
        "d0bd035a01eb4547bd84f7de6419fc76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d876bd69e91d4a1a8ca31ccdeb0889f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f210635fa4d44e4a98b8e7b5db26cf27",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 81131730,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 81131730,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_66c39a3d2fcf44938ca31ca7a18e7e21"
          }
        },
        "ff1b8ddef059409cb67c7cb23859bfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cacfb36f8350486b97651a3b0e8bc96c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.4M/77.4M [00:00&lt;00:00, 101MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76294465185d4f76ac4b429c0c429afe"
          }
        },
        "f210635fa4d44e4a98b8e7b5db26cf27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "66c39a3d2fcf44938ca31ca7a18e7e21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cacfb36f8350486b97651a3b0e8bc96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76294465185d4f76ac4b429c0c429afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sornavel/Hackathons-Janatahack-Jun-2020/blob/master/Image_Classification_15_Jun_densenet201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbHNnoSG1j9M",
        "colab_type": "code",
        "outputId": "ce773bff-ed97-4788-8d4b-e380899a1c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (47.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMxN4DD_1pVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S21HE-gZ1tf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "649tYSDnK-TB",
        "colab": {}
      },
      "source": [
        "download = drive.CreateFile({'id': '1-1swO1d7w-0-9I_CCX_yK64G638NfKCX'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En7hac61QIDs",
        "colab_type": "code",
        "outputId": "f84bfc4d-8410-408e-b209-10a5972caaa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "download.GetContentFile('train.zip')\n",
        "!unzip 'train.zip'\n",
        "train_df = pd.read_csv('train.csv')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\n",
            "replace images/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXvaaFeBXQRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "%matplotlib inline  \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from joblib import load, dump\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks import *\n",
        "from torchvision import models as md\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import re\n",
        "import math\n",
        "import collections\n",
        "from functools import partial\n",
        "from torch.utils import model_zoo\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghzD-xtuXYsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters for the entire model (stem, all blocks, and head)\n",
        "GlobalParams = collections.namedtuple('GlobalParams', [\n",
        "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
        "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
        "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
        "\n",
        "\n",
        "# Parameters for an individual model block\n",
        "BlockArgs = collections.namedtuple('BlockArgs', [\n",
        "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
        "    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
        "\n",
        "\n",
        "# Change namedtuple defaults\n",
        "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
        "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
        "\n",
        "\n",
        "def relu_fn(x):\n",
        "    \"\"\" Swish activation function \"\"\"\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def round_filters(filters, global_params):\n",
        "    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n",
        "    multiplier = global_params.width_coefficient\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    divisor = global_params.depth_divisor\n",
        "    min_depth = global_params.min_depth\n",
        "    filters *= multiplier\n",
        "    min_depth = min_depth or divisor\n",
        "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats, global_params):\n",
        "    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n",
        "    multiplier = global_params.depth_coefficient\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "\n",
        "def drop_connect(inputs, p, training):\n",
        "    \"\"\" Drop connect. \"\"\"\n",
        "    if not training: return inputs\n",
        "    batch_size = inputs.shape[0]\n",
        "    keep_prob = 1 - p\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_same_padding_conv2d(image_size=None):\n",
        "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
        "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
        "    if image_size is None:\n",
        "        return Conv2dDynamicSamePadding\n",
        "    else:\n",
        "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
        "\n",
        "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
        "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n",
        "\n",
        "    def forward(self, x):\n",
        "        ih, iw = x.size()[-2:]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class Conv2dStaticSamePadding(nn.Conv2d):\n",
        "    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "\n",
        "        # Calculate padding based on image size and save it\n",
        "        assert image_size is not None\n",
        "        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
        "        else:\n",
        "            self.static_padding = Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.static_padding(x)\n",
        "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input\n",
        "\n",
        "\n",
        "########################################################################\n",
        "############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def efficientnet_params(model_name):\n",
        "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
        "    params_dict = {\n",
        "        # Coefficients:   width,depth,res,dropout\n",
        "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "    }\n",
        "    return params_dict[model_name]\n",
        "\n",
        "\n",
        "class BlockDecoder(object):\n",
        "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_block_string(block_string):\n",
        "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
        "        assert isinstance(block_string, str)\n",
        "\n",
        "        ops = block_string.split('_')\n",
        "        options = {}\n",
        "        for op in ops:\n",
        "            splits = re.split(r'(\\d.*)', op)\n",
        "            if len(splits) >= 2:\n",
        "                key, value = splits[:2]\n",
        "                options[key] = value\n",
        "\n",
        "        # Check stride\n",
        "        assert (('s' in options and len(options['s']) == 1) or\n",
        "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
        "\n",
        "        return BlockArgs(\n",
        "            kernel_size=int(options['k']),\n",
        "            num_repeat=int(options['r']),\n",
        "            input_filters=int(options['i']),\n",
        "            output_filters=int(options['o']),\n",
        "            expand_ratio=int(options['e']),\n",
        "            id_skip=('noskip' not in block_string),\n",
        "            se_ratio=float(options['se']) if 'se' in options else None,\n",
        "            stride=[int(options['s'][0])])\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_block_string(block):\n",
        "        \"\"\"Encodes a block to a string.\"\"\"\n",
        "        args = [\n",
        "            'r%d' % block.num_repeat,\n",
        "            'k%d' % block.kernel_size,\n",
        "            's%d%d' % (block.strides[0], block.strides[1]),\n",
        "            'e%s' % block.expand_ratio,\n",
        "            'i%d' % block.input_filters,\n",
        "            'o%d' % block.output_filters\n",
        "        ]\n",
        "        if 0 < block.se_ratio <= 1:\n",
        "            args.append('se%s' % block.se_ratio)\n",
        "        if block.id_skip is False:\n",
        "            args.append('noskip')\n",
        "        return '_'.join(args)\n",
        "\n",
        "    @staticmethod\n",
        "    def decode(string_list):\n",
        "        \"\"\"\n",
        "        Decodes a list of string notations to specify blocks inside the network.\n",
        "\n",
        "        :param string_list: a list of strings, each string is a notation of block\n",
        "        :return: a list of BlockArgs namedtuples of block args\n",
        "        \"\"\"\n",
        "        assert isinstance(string_list, list)\n",
        "        blocks_args = []\n",
        "        for block_string in string_list:\n",
        "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
        "        return blocks_args\n",
        "\n",
        "    @staticmethod\n",
        "    def encode(blocks_args):\n",
        "        \"\"\"\n",
        "        Encodes a list of BlockArgs to a list of strings.\n",
        "\n",
        "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
        "        :return: a list of strings, each string is a notation of block\n",
        "        \"\"\"\n",
        "        block_strings = []\n",
        "        for block in blocks_args:\n",
        "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
        "        return block_strings\n",
        "\n",
        "\n",
        "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
        "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
        "    \"\"\" Creates a efficientnet model. \"\"\"\n",
        "\n",
        "    blocks_args = [\n",
        "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
        "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
        "    ]\n",
        "    blocks_args = BlockDecoder.decode(blocks_args)\n",
        "\n",
        "    global_params = GlobalParams(\n",
        "        batch_norm_momentum=0.99,\n",
        "        batch_norm_epsilon=1e-3,\n",
        "        dropout_rate=dropout_rate,\n",
        "        drop_connect_rate=drop_connect_rate,\n",
        "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
        "        num_classes=num_classes,\n",
        "        width_coefficient=width_coefficient,\n",
        "        depth_coefficient=depth_coefficient,\n",
        "        depth_divisor=8,\n",
        "        min_depth=None,\n",
        "        image_size=image_size,\n",
        "    )\n",
        "\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        w, d, s, p = efficientnet_params(model_name)\n",
        "        # note: all models have drop connect rate = 0.2\n",
        "        blocks_args, global_params = efficientnet(\n",
        "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
        "    else:\n",
        "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
        "    if override_params:\n",
        "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
        "        global_params = global_params._replace(**override_params)\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "url_map = {\n",
        "    'efficientnet-b0': 'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth',\n",
        "    'efficientnet-b1': 'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth',\n",
        "    'efficientnet-b2': 'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth',\n",
        "    'efficientnet-b3': 'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth',\n",
        "    'efficientnet-b4': 'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth',\n",
        "    'efficientnet-b5': 'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth',\n",
        "}\n",
        "\n",
        "def load_pretrained_weights(model, model_name, load_fc=True):\n",
        "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
        "    state_dict = model_zoo.load_url(url_map[model_name])\n",
        "    if load_fc:\n",
        "        model.load_state_dict(state_dict)\n",
        "    else:\n",
        "        state_dict.pop('_fc.weight')\n",
        "        state_dict.pop('_fc.bias')\n",
        "        res = model.load_state_dict(state_dict, strict=False)\n",
        "        assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n",
        "    print('Loaded pretrained weights for {}'.format(model_name))\n",
        "    \n",
        "    \n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mobile Inverted Residual Bottleneck Block\n",
        "\n",
        "    Args:\n",
        "        block_args (namedtuple): BlockArgs, see above\n",
        "        global_params (namedtuple): GlobalParam, see above\n",
        "\n",
        "    Attributes:\n",
        "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_args, global_params):\n",
        "        super().__init__()\n",
        "        self._block_args = block_args\n",
        "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
        "        self._bn_eps = global_params.batch_norm_epsilon\n",
        "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
        "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Expansion phase\n",
        "        inp = self._block_args.input_filters  # number of input channels\n",
        "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        k = self._block_args.kernel_size\n",
        "        s = self._block_args.stride\n",
        "        self._depthwise_conv = Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
        "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Output phase\n",
        "        final_oup = self._block_args.output_filters\n",
        "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "    def forward(self, inputs, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param inputs: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        x = inputs\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
        "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
        "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
        "            x = x + inputs  # skip connection\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    \"\"\"\n",
        "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
        "\n",
        "    Args:\n",
        "        blocks_args (list): A list of BlockArgs to construct blocks\n",
        "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
        "\n",
        "    Example:\n",
        "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__()\n",
        "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
        "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
        "        self._global_params = global_params\n",
        "        self._blocks_args = blocks_args\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Batch norm parameters\n",
        "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
        "        bn_eps = self._global_params.batch_norm_epsilon\n",
        "\n",
        "        # Stem\n",
        "        in_channels = 3  # rgb\n",
        "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
        "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
        "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Build blocks\n",
        "        self._blocks = nn.ModuleList([])\n",
        "        for block_args in self._blocks_args:\n",
        "\n",
        "            # Update block input and output filters based on depth multiplier.\n",
        "            block_args = block_args._replace(\n",
        "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
        "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
        "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
        "            )\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "            if block_args.num_repeat > 1:\n",
        "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
        "            for _ in range(block_args.num_repeat - 1):\n",
        "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "\n",
        "        # Head\n",
        "        in_channels = block_args.output_filters  # output of final block\n",
        "        out_channels = round_filters(1280, self._global_params)\n",
        "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Final linear layer\n",
        "        self._dropout = self._global_params.dropout_rate\n",
        "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "\n",
        "        # Stem\n",
        "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "\n",
        "        # Head\n",
        "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
        "\n",
        "        # Convolution layers\n",
        "        x = self.extract_features(inputs)\n",
        "\n",
        "        # Pooling and final linear layer\n",
        "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
        "        if self._dropout:\n",
        "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
        "        x = self._fc(x)\n",
        "        return x\n",
        "\n",
        "    @classmethod\n",
        "    def from_name(cls, model_name, override_params=None):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "        return EfficientNet(blocks_args, global_params)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, num_classes=1000):\n",
        "        model = EfficientNet.from_name(model_name, override_params={'num_classes': num_classes})\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def get_image_size(cls, model_name):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        _, _, res, _ = efficientnet_params(model_name)\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n",
        "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
        "        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n",
        "        num_models = 4 if also_need_pretrained_weights else 8\n",
        "        valid_models = ['efficientnet_b'+str(i) for i in range(num_models)]\n",
        "        if model_name.replace('-','_') not in valid_models:\n",
        "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRxCryxZSvM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#making model\n",
        "md_ef = EfficientNet.from_pretrained('efficientnet-b5', num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu44oAt3XfFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gammaCorrection(img_original, gamma):\n",
        "    lookUpTable = np.empty((1,256), np.uint8)\n",
        "    for i in range(256):\n",
        "        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
        "\n",
        "    res = cv2.LUT(img_original, lookUpTable)\n",
        "    return res;\n",
        "\n",
        "def cropImage(img):\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    ret,thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    rects = [cv2.boundingRect(cnt) for cnt in contours]\n",
        "\n",
        "    #Calculate the combined bounding rectangle points.\n",
        "    top_x = min([x for (x, y, w, h) in rects])\n",
        "    top_y = min([y for (x, y, w, h) in rects])\n",
        "    bottom_x = max([x+w for (x, y, w, h) in rects])\n",
        "    bottom_y = max([y+h for (x, y, w, h) in rects])\n",
        "\n",
        "    crop = gray[top_y:bottom_y,top_x:bottom_x]\n",
        "    crop = cv2.resize(crop,(800,600), interpolation=cv2.INTER_CUBIC)\n",
        "    return crop\n",
        "\n",
        "def preprocessImg(inputdir, outputdir):\n",
        "    imgList = os.listdir(inputdir)\n",
        "    listA = len(imgList)\n",
        "    gamma = 0.7\n",
        "    for i in range(listA):\n",
        "        img = cv2.imread(inputdir+imgList[i])\n",
        "        crop = cropImage(img)\n",
        "        new_image = gammaCorrection(crop,gamma)\n",
        "        #improve contrast of the image\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        cl1 = clahe.apply(new_image)\n",
        "        cv2.imwrite(str(outputdir+imgList[i]),cl1)\n",
        "        \n",
        "output_folder_train = os.path.join('../', 'output/train/')\n",
        "output_folder_test = os.path.join('../', 'output/test/')\n",
        "if not os.path.exists(output_folder_train):\n",
        "    os.makedirs(output_folder_train)\n",
        "if not os.path.exists(output_folder_test):\n",
        "    os.makedirs(output_folder_test)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SofCkx-JZpL3",
        "colab_type": "code",
        "outputId": "52fbcbfb-a36f-40a7-f0af-5cc0250c2b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "download = drive.CreateFile({'id': '1Kr8tXO_a0RhAO7cLA41a2SqE5w7vDC42'})\n",
        "download.GetContentFile('test.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_df.head()\n",
        "\n",
        "import cv2\n",
        "\n",
        "'''def get_df():\n",
        "    base_image_dir = os.path.join('.')\n",
        "    train_dir = os.path.join('images/')\n",
        "    test_dir = os.path.join('images/')\n",
        "    df = pd.read_csv('train.csv')\n",
        "#    df['path'] = df['image_names']\n",
        "#    df = df.drop(columns=['image_names'])\n",
        "    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    preprocessImg(train_dir,output_folder_train)\n",
        "    preprocessImg(test_dir,output_folder_test)\n",
        "    return df, test_df\n",
        "\n",
        "df, test_df = get_df()'''"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def get_df():\\n    base_image_dir = os.path.join('.')\\n    train_dir = os.path.join('images/')\\n    test_dir = os.path.join('images/')\\n    df = pd.read_csv('train.csv')\\n#    df['path'] = df['image_names']\\n#    df = df.drop(columns=['image_names'])\\n    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\\n    test_df = pd.read_csv('test.csv')\\n    preprocessImg(train_dir,output_folder_train)\\n    preprocessImg(test_dir,output_folder_test)\\n    return df, test_df\\n\\ndf, test_df = get_df()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voRdVHAEYRZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 16\n",
        "sz = 256\n",
        "tfms = get_transforms(do_flip=True,flip_vert=False, max_rotate=180,max_warp=0,max_zoom=1.1,max_lighting=0.1,p_lighting=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiWaRNplZtPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##transformations to be done to images\n",
        "tfms = get_transforms(do_flip=False,flip_vert=False ,max_rotate=10.0, max_zoom=1.22, max_lighting=0.22, max_warp=0.0, p_affine=0.75,\n",
        "                      p_lighting=0.75)\n",
        "#, xtra_tfms=zoom_crop(scale=(0.9,1.8), do_rand=True, p=0.8))\n",
        "\n",
        "## create databunch of test set to be passed\n",
        "test_img = ImageList.from_df(test_df, path='images/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMULU5nUXeNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(145)\n",
        "## create source of train image databunch\n",
        "src = (ImageList.from_df(train_df, path='images/')\n",
        "       .split_by_rand_pct(0.2)\n",
        "       #.split_none()\n",
        "       .label_from_df()\n",
        "       .add_test(test_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AqXvNdYRX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = (src.transform(tfms, size=256,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n",
        "        .databunch(path='.', bs=16, device= torch.device('cuda:0')).normalize(imagenet_stats))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtK0M3f7dBI",
        "colab_type": "code",
        "outputId": "3c658b9d-8e6b-495f-bdbc-f1ee5074fe62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "3382baf0eb914c7aba7a9ce8c2c78c7c",
            "d0bd035a01eb4547bd84f7de6419fc76",
            "d876bd69e91d4a1a8ca31ccdeb0889f6",
            "ff1b8ddef059409cb67c7cb23859bfa0",
            "f210635fa4d44e4a98b8e7b5db26cf27",
            "66c39a3d2fcf44938ca31ca7a18e7e21",
            "cacfb36f8350486b97651a3b0e8bc96c",
            "76294465185d4f76ac4b429c0c429afe"
          ]
        }
      },
      "source": [
        "\n",
        "\n",
        "'''from fastai.metrics import error_rate # 1 - accuracy\n",
        "\n",
        "learn = Learner(data, \n",
        "                md_ef, \n",
        "                metrics=[FBeta(beta=1, average='macro'), accuracy], \n",
        "                model_dir=\"models\")\n",
        "\n",
        "learn.data.add_test(ImageList.from_df(test_df,\n",
        "                                      './',\n",
        "                                      folder='images',\n",
        "                                      suffix=''))'''\n",
        "\n",
        "learn = cnn_learner(data=data, base_arch=models.densenet201, metrics=[FBeta(beta=1, average='macro'), accuracy],\n",
        "                    callback_fns=ShowGraph)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/checkpoints/densenet201-c1103571.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3382baf0eb914c7aba7a9ce8c2c78c7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=81131730.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG1WjSZjaFZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn.lr_find()\n",
        "#learn.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvpqZwRCaaUq",
        "colab_type": "code",
        "outputId": "79baa4f0-a499-46eb-bae2-92877788309b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMZHDjWaSY7U",
        "colab_type": "code",
        "outputId": "abed7461-1f60-41f4-e71d-f351f092dfde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "#lr = 2.75E-06\n",
        "learn.fit_one_cycle(10, 1e-03)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>f_beta</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.459849</td>\n",
              "      <td>0.202648</td>\n",
              "      <td>0.920621</td>\n",
              "      <td>0.924012</td>\n",
              "      <td>00:55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.350369</td>\n",
              "      <td>0.384099</td>\n",
              "      <td>0.897515</td>\n",
              "      <td>0.899696</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.266298</td>\n",
              "      <td>0.286862</td>\n",
              "      <td>0.920826</td>\n",
              "      <td>0.924012</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.251324</td>\n",
              "      <td>0.194978</td>\n",
              "      <td>0.943831</td>\n",
              "      <td>0.945289</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.142991</td>\n",
              "      <td>0.158464</td>\n",
              "      <td>0.955606</td>\n",
              "      <td>0.957447</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.112748</td>\n",
              "      <td>0.186991</td>\n",
              "      <td>0.939828</td>\n",
              "      <td>0.942249</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.103790</td>\n",
              "      <td>0.130835</td>\n",
              "      <td>0.949264</td>\n",
              "      <td>0.951368</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.078583</td>\n",
              "      <td>0.168308</td>\n",
              "      <td>0.949130</td>\n",
              "      <td>0.951368</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.052912</td>\n",
              "      <td>0.196289</td>\n",
              "      <td>0.945878</td>\n",
              "      <td>0.948328</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.045286</td>\n",
              "      <td>0.186923</td>\n",
              "      <td>0.955606</td>\n",
              "      <td>0.957447</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIn9R2Ad9mT7",
        "colab_type": "code",
        "outputId": "e83aabba-2973-4974-85b0-414eec8f7a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "preds,y = learn.get_preds(DatasetType.Test)\n",
        "preds,y = learn.TTA(ds_type=DatasetType.Test, scale=1.0)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxRVkoMOrC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelled_preds = [np.argmax(preds[i]) for i in range(len(preds))]\n",
        "labelled_preds = np.array(labelled_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0aTWIoAC6jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create submission file\n",
        "df = pd.DataFrame({'image_names':test_df['image_names'], 'emergency_or_not':labelled_preds}, columns=['image_names', 'emergency_or_not'])\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWECtuO6F-ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}