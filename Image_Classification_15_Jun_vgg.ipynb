{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification_15-Jun.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff042eb2891d4184b9d35e867c4ff8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2dcffc2c5d3f46d289f52b3e7e3e1130",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b87c94aae7541eb9323560b124d719b",
              "IPY_MODEL_cb4249e9bc1443dfbe2ef91925d56831"
            ]
          }
        },
        "2dcffc2c5d3f46d289f52b3e7e3e1130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b87c94aae7541eb9323560b124d719b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_881d99d677984d81bd20f9c57ab6ab9c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574769405,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574769405,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad7255fd7b5447efa02847bc4a74f8a5"
          }
        },
        "cb4249e9bc1443dfbe2ef91925d56831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9189e856bf5c4a2db711403b86433deb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:04&lt;00:00, 142MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e643bb082e8a4cb297bd3dd56053cc7f"
          }
        },
        "881d99d677984d81bd20f9c57ab6ab9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad7255fd7b5447efa02847bc4a74f8a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9189e856bf5c4a2db711403b86433deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e643bb082e8a4cb297bd3dd56053cc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sornavel/Hackathons-Janatahack-Jun-2020/blob/master/Image_Classification_15_Jun_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbHNnoSG1j9M",
        "colab_type": "code",
        "outputId": "ce773bff-ed97-4788-8d4b-e380899a1c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (47.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMxN4DD_1pVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S21HE-gZ1tf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "649tYSDnK-TB",
        "colab": {}
      },
      "source": [
        "download = drive.CreateFile({'id': '1-1swO1d7w-0-9I_CCX_yK64G638NfKCX'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En7hac61QIDs",
        "colab_type": "code",
        "outputId": "f84bfc4d-8410-408e-b209-10a5972caaa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "download.GetContentFile('train.zip')\n",
        "!unzip 'train.zip'\n",
        "train_df = pd.read_csv('train.csv')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\n",
            "replace images/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXvaaFeBXQRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "%matplotlib inline  \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from joblib import load, dump\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.callbacks import *\n",
        "from torchvision import models as md\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import re\n",
        "import math\n",
        "import collections\n",
        "from functools import partial\n",
        "from torch.utils import model_zoo\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghzD-xtuXYsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters for the entire model (stem, all blocks, and head)\n",
        "GlobalParams = collections.namedtuple('GlobalParams', [\n",
        "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
        "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
        "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
        "\n",
        "\n",
        "# Parameters for an individual model block\n",
        "BlockArgs = collections.namedtuple('BlockArgs', [\n",
        "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
        "    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
        "\n",
        "\n",
        "# Change namedtuple defaults\n",
        "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
        "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
        "\n",
        "\n",
        "def relu_fn(x):\n",
        "    \"\"\" Swish activation function \"\"\"\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def round_filters(filters, global_params):\n",
        "    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n",
        "    multiplier = global_params.width_coefficient\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    divisor = global_params.depth_divisor\n",
        "    min_depth = global_params.min_depth\n",
        "    filters *= multiplier\n",
        "    min_depth = min_depth or divisor\n",
        "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats, global_params):\n",
        "    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n",
        "    multiplier = global_params.depth_coefficient\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "\n",
        "def drop_connect(inputs, p, training):\n",
        "    \"\"\" Drop connect. \"\"\"\n",
        "    if not training: return inputs\n",
        "    batch_size = inputs.shape[0]\n",
        "    keep_prob = 1 - p\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_same_padding_conv2d(image_size=None):\n",
        "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
        "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
        "    if image_size is None:\n",
        "        return Conv2dDynamicSamePadding\n",
        "    else:\n",
        "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
        "\n",
        "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
        "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n",
        "\n",
        "    def forward(self, x):\n",
        "        ih, iw = x.size()[-2:]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class Conv2dStaticSamePadding(nn.Conv2d):\n",
        "    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "\n",
        "        # Calculate padding based on image size and save it\n",
        "        assert image_size is not None\n",
        "        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
        "        else:\n",
        "            self.static_padding = Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.static_padding(x)\n",
        "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input\n",
        "\n",
        "\n",
        "########################################################################\n",
        "############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def efficientnet_params(model_name):\n",
        "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
        "    params_dict = {\n",
        "        # Coefficients:   width,depth,res,dropout\n",
        "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "    }\n",
        "    return params_dict[model_name]\n",
        "\n",
        "\n",
        "class BlockDecoder(object):\n",
        "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_block_string(block_string):\n",
        "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
        "        assert isinstance(block_string, str)\n",
        "\n",
        "        ops = block_string.split('_')\n",
        "        options = {}\n",
        "        for op in ops:\n",
        "            splits = re.split(r'(\\d.*)', op)\n",
        "            if len(splits) >= 2:\n",
        "                key, value = splits[:2]\n",
        "                options[key] = value\n",
        "\n",
        "        # Check stride\n",
        "        assert (('s' in options and len(options['s']) == 1) or\n",
        "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
        "\n",
        "        return BlockArgs(\n",
        "            kernel_size=int(options['k']),\n",
        "            num_repeat=int(options['r']),\n",
        "            input_filters=int(options['i']),\n",
        "            output_filters=int(options['o']),\n",
        "            expand_ratio=int(options['e']),\n",
        "            id_skip=('noskip' not in block_string),\n",
        "            se_ratio=float(options['se']) if 'se' in options else None,\n",
        "            stride=[int(options['s'][0])])\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_block_string(block):\n",
        "        \"\"\"Encodes a block to a string.\"\"\"\n",
        "        args = [\n",
        "            'r%d' % block.num_repeat,\n",
        "            'k%d' % block.kernel_size,\n",
        "            's%d%d' % (block.strides[0], block.strides[1]),\n",
        "            'e%s' % block.expand_ratio,\n",
        "            'i%d' % block.input_filters,\n",
        "            'o%d' % block.output_filters\n",
        "        ]\n",
        "        if 0 < block.se_ratio <= 1:\n",
        "            args.append('se%s' % block.se_ratio)\n",
        "        if block.id_skip is False:\n",
        "            args.append('noskip')\n",
        "        return '_'.join(args)\n",
        "\n",
        "    @staticmethod\n",
        "    def decode(string_list):\n",
        "        \"\"\"\n",
        "        Decodes a list of string notations to specify blocks inside the network.\n",
        "\n",
        "        :param string_list: a list of strings, each string is a notation of block\n",
        "        :return: a list of BlockArgs namedtuples of block args\n",
        "        \"\"\"\n",
        "        assert isinstance(string_list, list)\n",
        "        blocks_args = []\n",
        "        for block_string in string_list:\n",
        "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
        "        return blocks_args\n",
        "\n",
        "    @staticmethod\n",
        "    def encode(blocks_args):\n",
        "        \"\"\"\n",
        "        Encodes a list of BlockArgs to a list of strings.\n",
        "\n",
        "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
        "        :return: a list of strings, each string is a notation of block\n",
        "        \"\"\"\n",
        "        block_strings = []\n",
        "        for block in blocks_args:\n",
        "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
        "        return block_strings\n",
        "\n",
        "\n",
        "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
        "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
        "    \"\"\" Creates a efficientnet model. \"\"\"\n",
        "\n",
        "    blocks_args = [\n",
        "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
        "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
        "    ]\n",
        "    blocks_args = BlockDecoder.decode(blocks_args)\n",
        "\n",
        "    global_params = GlobalParams(\n",
        "        batch_norm_momentum=0.99,\n",
        "        batch_norm_epsilon=1e-3,\n",
        "        dropout_rate=dropout_rate,\n",
        "        drop_connect_rate=drop_connect_rate,\n",
        "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
        "        num_classes=num_classes,\n",
        "        width_coefficient=width_coefficient,\n",
        "        depth_coefficient=depth_coefficient,\n",
        "        depth_divisor=8,\n",
        "        min_depth=None,\n",
        "        image_size=image_size,\n",
        "    )\n",
        "\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        w, d, s, p = efficientnet_params(model_name)\n",
        "        # note: all models have drop connect rate = 0.2\n",
        "        blocks_args, global_params = efficientnet(\n",
        "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
        "    else:\n",
        "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
        "    if override_params:\n",
        "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
        "        global_params = global_params._replace(**override_params)\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "url_map = {\n",
        "    'efficientnet-b0': 'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth',\n",
        "    'efficientnet-b1': 'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth',\n",
        "    'efficientnet-b2': 'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth',\n",
        "    'efficientnet-b3': 'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth',\n",
        "    'efficientnet-b4': 'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth',\n",
        "    'efficientnet-b5': 'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth',\n",
        "}\n",
        "\n",
        "def load_pretrained_weights(model, model_name, load_fc=True):\n",
        "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
        "    state_dict = model_zoo.load_url(url_map[model_name])\n",
        "    if load_fc:\n",
        "        model.load_state_dict(state_dict)\n",
        "    else:\n",
        "        state_dict.pop('_fc.weight')\n",
        "        state_dict.pop('_fc.bias')\n",
        "        res = model.load_state_dict(state_dict, strict=False)\n",
        "        assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n",
        "    print('Loaded pretrained weights for {}'.format(model_name))\n",
        "    \n",
        "    \n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mobile Inverted Residual Bottleneck Block\n",
        "\n",
        "    Args:\n",
        "        block_args (namedtuple): BlockArgs, see above\n",
        "        global_params (namedtuple): GlobalParam, see above\n",
        "\n",
        "    Attributes:\n",
        "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_args, global_params):\n",
        "        super().__init__()\n",
        "        self._block_args = block_args\n",
        "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
        "        self._bn_eps = global_params.batch_norm_epsilon\n",
        "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
        "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Expansion phase\n",
        "        inp = self._block_args.input_filters  # number of input channels\n",
        "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        k = self._block_args.kernel_size\n",
        "        s = self._block_args.stride\n",
        "        self._depthwise_conv = Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
        "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Output phase\n",
        "        final_oup = self._block_args.output_filters\n",
        "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "    def forward(self, inputs, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param inputs: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        x = inputs\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
        "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
        "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
        "            x = x + inputs  # skip connection\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    \"\"\"\n",
        "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
        "\n",
        "    Args:\n",
        "        blocks_args (list): A list of BlockArgs to construct blocks\n",
        "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
        "\n",
        "    Example:\n",
        "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__()\n",
        "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
        "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
        "        self._global_params = global_params\n",
        "        self._blocks_args = blocks_args\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Batch norm parameters\n",
        "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
        "        bn_eps = self._global_params.batch_norm_epsilon\n",
        "\n",
        "        # Stem\n",
        "        in_channels = 3  # rgb\n",
        "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
        "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
        "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Build blocks\n",
        "        self._blocks = nn.ModuleList([])\n",
        "        for block_args in self._blocks_args:\n",
        "\n",
        "            # Update block input and output filters based on depth multiplier.\n",
        "            block_args = block_args._replace(\n",
        "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
        "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
        "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
        "            )\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "            if block_args.num_repeat > 1:\n",
        "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
        "            for _ in range(block_args.num_repeat - 1):\n",
        "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "\n",
        "        # Head\n",
        "        in_channels = block_args.output_filters  # output of final block\n",
        "        out_channels = round_filters(1280, self._global_params)\n",
        "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Final linear layer\n",
        "        self._dropout = self._global_params.dropout_rate\n",
        "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "\n",
        "        # Stem\n",
        "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "\n",
        "        # Head\n",
        "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
        "\n",
        "        # Convolution layers\n",
        "        x = self.extract_features(inputs)\n",
        "\n",
        "        # Pooling and final linear layer\n",
        "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
        "        if self._dropout:\n",
        "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
        "        x = self._fc(x)\n",
        "        return x\n",
        "\n",
        "    @classmethod\n",
        "    def from_name(cls, model_name, override_params=None):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "        return EfficientNet(blocks_args, global_params)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, num_classes=1000):\n",
        "        model = EfficientNet.from_name(model_name, override_params={'num_classes': num_classes})\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def get_image_size(cls, model_name):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        _, _, res, _ = efficientnet_params(model_name)\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n",
        "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
        "        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n",
        "        num_models = 4 if also_need_pretrained_weights else 8\n",
        "        valid_models = ['efficientnet_b'+str(i) for i in range(num_models)]\n",
        "        if model_name.replace('-','_') not in valid_models:\n",
        "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRxCryxZSvM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#making model\n",
        "md_ef = EfficientNet.from_pretrained('efficientnet-b5', num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu44oAt3XfFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gammaCorrection(img_original, gamma):\n",
        "    lookUpTable = np.empty((1,256), np.uint8)\n",
        "    for i in range(256):\n",
        "        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
        "\n",
        "    res = cv2.LUT(img_original, lookUpTable)\n",
        "    return res;\n",
        "\n",
        "def cropImage(img):\n",
        "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    ret,thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    rects = [cv2.boundingRect(cnt) for cnt in contours]\n",
        "\n",
        "    #Calculate the combined bounding rectangle points.\n",
        "    top_x = min([x for (x, y, w, h) in rects])\n",
        "    top_y = min([y for (x, y, w, h) in rects])\n",
        "    bottom_x = max([x+w for (x, y, w, h) in rects])\n",
        "    bottom_y = max([y+h for (x, y, w, h) in rects])\n",
        "\n",
        "    crop = gray[top_y:bottom_y,top_x:bottom_x]\n",
        "    crop = cv2.resize(crop,(800,600), interpolation=cv2.INTER_CUBIC)\n",
        "    return crop\n",
        "\n",
        "def preprocessImg(inputdir, outputdir):\n",
        "    imgList = os.listdir(inputdir)\n",
        "    listA = len(imgList)\n",
        "    gamma = 0.7\n",
        "    for i in range(listA):\n",
        "        img = cv2.imread(inputdir+imgList[i])\n",
        "        crop = cropImage(img)\n",
        "        new_image = gammaCorrection(crop,gamma)\n",
        "        #improve contrast of the image\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        cl1 = clahe.apply(new_image)\n",
        "        cv2.imwrite(str(outputdir+imgList[i]),cl1)\n",
        "        \n",
        "output_folder_train = os.path.join('../', 'output/train/')\n",
        "output_folder_test = os.path.join('../', 'output/test/')\n",
        "if not os.path.exists(output_folder_train):\n",
        "    os.makedirs(output_folder_train)\n",
        "if not os.path.exists(output_folder_test):\n",
        "    os.makedirs(output_folder_test)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SofCkx-JZpL3",
        "colab_type": "code",
        "outputId": "52fbcbfb-a36f-40a7-f0af-5cc0250c2b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "download = drive.CreateFile({'id': '1Kr8tXO_a0RhAO7cLA41a2SqE5w7vDC42'})\n",
        "download.GetContentFile('test.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_df.head()\n",
        "\n",
        "import cv2\n",
        "\n",
        "'''def get_df():\n",
        "    base_image_dir = os.path.join('.')\n",
        "    train_dir = os.path.join('images/')\n",
        "    test_dir = os.path.join('images/')\n",
        "    df = pd.read_csv('train.csv')\n",
        "#    df['path'] = df['image_names']\n",
        "#    df = df.drop(columns=['image_names'])\n",
        "    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    preprocessImg(train_dir,output_folder_train)\n",
        "    preprocessImg(test_dir,output_folder_test)\n",
        "    return df, test_df\n",
        "\n",
        "df, test_df = get_df()'''"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def get_df():\\n    base_image_dir = os.path.join('.')\\n    train_dir = os.path.join('images/')\\n    test_dir = os.path.join('images/')\\n    df = pd.read_csv('train.csv')\\n#    df['path'] = df['image_names']\\n#    df = df.drop(columns=['image_names'])\\n    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\\n    test_df = pd.read_csv('test.csv')\\n    preprocessImg(train_dir,output_folder_train)\\n    preprocessImg(test_dir,output_folder_test)\\n    return df, test_df\\n\\ndf, test_df = get_df()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voRdVHAEYRZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 16\n",
        "sz = 256\n",
        "tfms = get_transforms(do_flip=True,flip_vert=False, max_rotate=180,max_warp=0,max_zoom=1.1,max_lighting=0.1,p_lighting=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiWaRNplZtPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##transformations to be done to images\n",
        "tfms = get_transforms(do_flip=False,flip_vert=False ,max_rotate=10.0, max_zoom=1.22, max_lighting=0.22, max_warp=0.0, p_affine=0.75,\n",
        "                      p_lighting=0.75)\n",
        "#, xtra_tfms=zoom_crop(scale=(0.9,1.8), do_rand=True, p=0.8))\n",
        "\n",
        "## create databunch of test set to be passed\n",
        "test_img = ImageList.from_df(test_df, path='images/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMULU5nUXeNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(145)\n",
        "## create source of train image databunch\n",
        "src = (ImageList.from_df(train_df, path='images/')\n",
        "       .split_by_rand_pct(0.2)\n",
        "       #.split_none()\n",
        "       .label_from_df()\n",
        "       .add_test(test_img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0AqXvNdYRX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = (src.transform(tfms, size=256,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n",
        "        .databunch(path='.', bs=16, device= torch.device('cuda:0')).normalize(imagenet_stats))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtK0M3f7dBI",
        "colab_type": "code",
        "outputId": "928b64e7-eb2c-4dd4-ab21-c9ec59e9b3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "ff042eb2891d4184b9d35e867c4ff8d0",
            "2dcffc2c5d3f46d289f52b3e7e3e1130",
            "2b87c94aae7541eb9323560b124d719b",
            "cb4249e9bc1443dfbe2ef91925d56831",
            "881d99d677984d81bd20f9c57ab6ab9c",
            "ad7255fd7b5447efa02847bc4a74f8a5",
            "9189e856bf5c4a2db711403b86433deb",
            "e643bb082e8a4cb297bd3dd56053cc7f"
          ]
        }
      },
      "source": [
        "\n",
        "\n",
        "'''from fastai.metrics import error_rate # 1 - accuracy\n",
        "\n",
        "learn = Learner(data, \n",
        "                md_ef, \n",
        "                metrics=[FBeta(beta=1, average='macro'), accuracy], \n",
        "                model_dir=\"models\")\n",
        "\n",
        "learn.data.add_test(ImageList.from_df(test_df,\n",
        "                                      './',\n",
        "                                      folder='images',\n",
        "                                      suffix=''))'''\n",
        "\n",
        "learn = cnn_learner(data=data, base_arch=models.vgg19_bn, metrics=[FBeta(beta=1, average='macro'), accuracy],\n",
        "                    callback_fns=ShowGraph)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/checkpoints/vgg19_bn-c79401a0.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff042eb2891d4184b9d35e867c4ff8d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574769405.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG1WjSZjaFZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learn.lr_find()\n",
        "#learn.recorder.plot(suggestion=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvpqZwRCaaUq",
        "colab_type": "code",
        "outputId": "48922562-6d3a-4e24-fe35-a21004f9a4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5824"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMZHDjWaSY7U",
        "colab_type": "code",
        "outputId": "ff9d2321-c045-44ab-e7f4-cb6ccd5a02d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "#lr = 2.75E-06\n",
        "learn.fit_one_cycle(10, 1e-03)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>f_beta</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.714872</td>\n",
              "      <td>0.252379</td>\n",
              "      <td>0.893685</td>\n",
              "      <td>0.896657</td>\n",
              "      <td>00:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.437268</td>\n",
              "      <td>0.192962</td>\n",
              "      <td>0.924089</td>\n",
              "      <td>0.927052</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.333731</td>\n",
              "      <td>0.178656</td>\n",
              "      <td>0.946162</td>\n",
              "      <td>0.948328</td>\n",
              "      <td>00:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.225586</td>\n",
              "      <td>0.209085</td>\n",
              "      <td>0.934266</td>\n",
              "      <td>0.936170</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.243931</td>\n",
              "      <td>0.129967</td>\n",
              "      <td>0.946426</td>\n",
              "      <td>0.948328</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.189608</td>\n",
              "      <td>0.156827</td>\n",
              "      <td>0.946296</td>\n",
              "      <td>0.948328</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.143203</td>\n",
              "      <td>0.154512</td>\n",
              "      <td>0.949130</td>\n",
              "      <td>0.951368</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.118930</td>\n",
              "      <td>0.129464</td>\n",
              "      <td>0.958829</td>\n",
              "      <td>0.960486</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.117587</td>\n",
              "      <td>0.125706</td>\n",
              "      <td>0.958932</td>\n",
              "      <td>0.960486</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.081635</td>\n",
              "      <td>0.140308</td>\n",
              "      <td>0.952373</td>\n",
              "      <td>0.954407</td>\n",
              "      <td>00:47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIn9R2Ad9mT7",
        "colab_type": "code",
        "outputId": "8d61fda7-816e-4c92-eb47-bf0f1ff1db1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "preds,y = learn.get_preds(DatasetType.Test)\n",
        "preds,y = learn.TTA(ds_type=DatasetType.Test, scale=1.0)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVxRVkoMOrC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labelled_preds = [np.argmax(preds[i]) for i in range(len(preds))]\n",
        "labelled_preds = np.array(labelled_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0aTWIoAC6jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create submission file\n",
        "df = pd.DataFrame({'image_names':test_df['image_names'], 'emergency_or_not':labelled_preds}, columns=['image_names', 'emergency_or_not'])\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWECtuO6F-ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}